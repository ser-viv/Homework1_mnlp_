{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1nfcHTMs9b59d3A8L8Due",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ser-viv/Homework1_mnlp_/blob/main/Homework1_mnlp_Rubber_Ducklings_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload the dataset (two ways)"
      ],
      "metadata": {
        "id": "xmocN0a34jw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#avoid conflicts and install datasets\n",
        "!pip install datasets --quiet\n",
        "\n",
        "!mkdir dataset\n",
        "%cd dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRQrg_uS-_se",
        "outputId": "f5b4361b-f36c-4490-d23e-e6ede1918028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Upload from Huggingface"
      ],
      "metadata": {
        "id": "4uSEV4ZC8Zpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change token\n",
        "login(token=\"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\")\n",
        "# Change dataset path\n",
        "dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset')\n",
        "# Change key to accept to desider set [train, validation, test]\n",
        "dataset =dataset['validation']\n",
        "dataset"
      ],
      "metadata": {
        "id": "y49wQW4a48TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload from files"
      ],
      "metadata": {
        "id": "w9A_YePF9sNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we can upload the dataset from file as .csv\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "df = pd.read_csv(filename)\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "cCnUScUf546y",
        "outputId": "bec6584b-1b57-465e-d568-2e0bff711adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f9fce169-3b69-4541-a564-bf26c02d7eab\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f9fce169-3b69-4541-a564-bf26c02d7eab\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving nome_del_tuo_file.csv to nome_del_tuo_file.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['item', 'name', 'description', 'type', 'category', 'subcategory', 'label'],\n",
              "    num_rows: 300\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non LLM based approach"
      ],
      "metadata": {
        "id": "3d90JlecGuhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#here we do the necessary imports\n",
        "!pip install wikidata --quiet\n",
        "\n",
        "from wikidata.client import Client\n",
        "client = Client()\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import requests\n",
        "\n",
        "import re\n",
        "from json import JSONDecodeError\n",
        "\n",
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "\n",
        "from IPython.display import display, Image, clear_output\n",
        "import time\n",
        "from io import BytesIO\n",
        "\n",
        "secret_url1 = \"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.imgflip.com%2F26bkez.jpg&f=1&nofb=1&ipt=5184b45de5eab957826b135071574e7d0c7a3f2e7aae2de7ab8a601f621d7198\"  # Sostituisci con l'URL effettivo dell'immagine\n",
        "response = requests.get(secret_url1)\n",
        "\n",
        "\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "qb3Pc0VSI7q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "ucbaOWjcQLgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UTILS FUNCTIONS\n",
        "\n",
        "# this function extracts the id from a dataset sample\n",
        "def get_wikidata_id(sample_):\n",
        "  url = sample_['item']\n",
        "  return url.split('/')[-1]\n",
        "\n",
        "# this function extracts the description from a dataset sample\n",
        "def get_wikidata_description(sample_):\n",
        "  desc = sample_['description']\n",
        "  return desc\n",
        "\n",
        "# this function gets the name of the entity from an id\n",
        "def get_entity_from_id(id_):\n",
        "  item = client.get(id_, load=True)\n",
        "  name = str(item.label)\n",
        "  return name\n",
        "\n",
        "# this function gets the id from a name\n",
        "def get_wikidata_id_from_name(entity_name):\n",
        "    url = \"https://www.wikidata.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"wbsearchentities\",\n",
        "        \"format\": \"json\",\n",
        "        \"language\": \"en\",\n",
        "        \"search\": entity_name,\n",
        "        \"limit\": 1\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "    if data['search']:\n",
        "        return data['search'][0]['id']\n",
        "    else:\n",
        "        ''\n",
        "\n",
        "# this function extracts geopolitical entities from the description of an entity\n",
        "def extract_GPE(doc_):\n",
        "  GPEs_list = []\n",
        "  for ent in doc_.ents:\n",
        "    if (ent.label_ == 'GPE') :\n",
        "       list_check = []\n",
        "       list_check.append(ent.text)\n",
        "       GPEs_list = checklist(list_check,GPEs_list)\n",
        "  return GPEs_list\n",
        "\n",
        "# finds the nationality of autor\n",
        "def get_author_GPE(doc_):\n",
        "  GPEs_list = []\n",
        "  for ent in doc_.ents:\n",
        "    if (ent.label_ == 'GPE') :\n",
        "      GPEs_list.append(ent.text)\n",
        "  return GPEs_list\n",
        "\n",
        "# this extract the names of the entities metioned in a given claim\n",
        "def extract_from_claim(claim_,):\n",
        "  claim_list = []\n",
        "  for cl in claim_:\n",
        "      id = cl['mainsnak']['datavalue']['value']['id']\n",
        "      name = get_entity_from_id(id)\n",
        "      claim_list.append(name)\n",
        "  return claim_list\n",
        "\n",
        "# this function takes the title of the wikipedia page given the id\n",
        "def get_wikipedia_title(id):\n",
        "  try:\n",
        "    item = client.get(id, load=True)\n",
        "    sitelinks = item.data.get(\"sitelinks\", {}) #get the sitelinks\n",
        "    try:\n",
        "      enwiki = sitelinks.get(\"enwiki\") #find wikipedia page\n",
        "    except AttributeError:\n",
        "      return ''\n",
        "    if enwiki is not None:\n",
        "          title = enwiki['title']\n",
        "          return title\n",
        "    else:\n",
        "          return ''\n",
        "  except (JSONDecodeError, requests.exceptions.RequestException, KeyError) as e:\n",
        "    return ''\n",
        "\n",
        "# gets the description of the wikipedia page from title\n",
        "def get_wikipedia_desc_from_title(title,n):\n",
        "    if(title==''):\n",
        "      return ''\n",
        "    api_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": True,\n",
        "        \"titles\": title,\n",
        "        \"format\": \"json\",\n",
        "        \"redirects\": 1\n",
        "    }\n",
        "    try:\n",
        "      res = requests.get(api_url, params=params).json()\n",
        "    except JSONDecodeError:\n",
        "      print('error')\n",
        "      return ''\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      return ''\n",
        "\n",
        "# this function takes 2 list, the first has entities we want to add to list 2 if there isn't one already\n",
        "def checklist(list1,list2):\n",
        "  for i in list1:\n",
        "    if not(i in list2) and i[0].isupper():\n",
        "      list2.append(i)\n",
        "  return list2\n",
        "\n",
        "# this function finds an id from a sample of dataset\n",
        "def extract_sampleid(sample_):\n",
        "    url = sample_['item']\n",
        "    id = url.strip().split(\"/\")[-1]\n",
        "    return id\n",
        "\n",
        "# this function returns the history section of a wikipedia page given the text\n",
        "def find_history_wiki(text_):\n",
        "  match = re.search(r\"=* History ==*\\n(.*?)\\n==* \", text_, re.DOTALL)\n",
        "  if not match:\n",
        "    return ''\n",
        "  else:\n",
        "    return match[1]\n",
        "\n",
        "# this function takes a wikipedia text given the title\n",
        "def get_wikipedia_text_from_title(title):\n",
        "    if(title==''):\n",
        "      return ''\n",
        "    api_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": True,\n",
        "        \"titles\": title,\n",
        "        \"format\": \"json\",\n",
        "        \"redirects\": 1\n",
        "    }\n",
        "    try:\n",
        "      res = requests.get(api_url, params=params).json()\n",
        "    except JSONDecodeError:\n",
        "      print('error')\n",
        "      return ''\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      return ''\n",
        "\n",
        "    page = next(iter(res[\"query\"][\"pages\"].values()))\n",
        "    text = page.get(\"extract\", \"\")\n",
        "\n",
        "    return text\n",
        "\n",
        "# finds history section of wikipedia from an id\n",
        "def get_history_from_id(id_):\n",
        "  if id_ is None:\n",
        "    return ''\n",
        "  title = get_wikipedia_title(id_)\n",
        "  text = get_wikipedia_text_from_title(title)\n",
        "  history = find_history_wiki(text)\n",
        "  if history:\n",
        "    return history\n",
        "  else:\n",
        "    return ''\n",
        "\n",
        "# finds if in the history section of wikipedia more countries are mentioned\n",
        "def history_popularity(id_):\n",
        "  history = get_history_from_id(id_)\n",
        "  doc = nlp(history)\n",
        "  gpe_list = extract_GPE(doc)\n",
        "  if len(gpe_list)>2:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "#this get a wikipedia title from the wikidata id\n",
        "def get_wikipedia_title(id):\n",
        "  try:\n",
        "    item = client.get(id, load=True)\n",
        "    sitelinks = item.data.get(\"sitelinks\", {}) #get the sitelinks\n",
        "    try:\n",
        "      enwiki = sitelinks.get(\"enwiki\") #find wikipedia page\n",
        "    except AttributeError:\n",
        "      return ''\n",
        "    if enwiki is not None:\n",
        "          title = enwiki['title']\n",
        "          return title\n",
        "    else:\n",
        "          return ''\n",
        "  except (JSONDecodeError, requests.exceptions.RequestException, KeyError) as e:\n",
        "    return ''\n",
        "#this gets an url from a title\n",
        "def get_wikipedia_url_from_title(title):\n",
        "  url_general = \"https://en.wikipedia.org/wiki/\"\n",
        "  url = url_general + title\n",
        "  return url\n",
        "\n",
        "\n",
        "#this function returns a list of headers starting from a wikipedia url\n",
        "def find_wiki_headers_from_id(id):\n",
        "  title = get_wikipedia_title(id)\n",
        "  url = get_wikipedia_url_from_title(title)\n",
        "  page = requests.get(url)\n",
        "  data = page.text\n",
        "  soup = BeautifulSoup(data)\n",
        "\n",
        "  headers = soup.find_all([f'h{i}' for i in range(1,7) ])\n",
        "  headers_titles= [elem.text for elem in headers]\n",
        "  return headers_titles\n",
        "\n",
        "#this function returns true if there is an history section\n",
        "def is_there_history(id):\n",
        "  headers = find_wiki_headers_from_id(id)\n",
        "  if 'History' in headers:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "#this function returns true if there are sections related to adaptations\n",
        "def are_there_adaptations(id):\n",
        "  headers = find_wiki_headers_from_id(id)\n",
        "  if 'Adaptations' in headers:\n",
        "    return True\n",
        "  if 'In other media' in headers:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "metadata": {
        "id": "o4aMaA5iHeaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function used in core function for classifications\n",
        "\n",
        "#this function takes as input an entity id (a string, e.g. \"Q483444\") and returns the country of origin of an entity (if there is one)\n",
        "def where_is_it_from(item_id):\n",
        "  list_ = []\n",
        "  item = client.get(item_id, load=True) #item di wikidata\n",
        "  claims = item.data.get(\"claims\", {}) #claims associatiated to item\n",
        "  country_of_origin_claims = claims.get(\"P495\", []) #the specific claim associated to country of origin (see above)\n",
        "  indigenous_claim = claims.get(\"P2341\",[])\n",
        "  person_origin_claim = claims.get(\"P27\",[])\n",
        "  country_claims = claims.get(\"P17\", [])\n",
        "\n",
        "  if person_origin_claim:\n",
        "    #print('In origin claim')\n",
        "    countrylist = extract_from_claim(person_origin_claim)\n",
        "    list_= checklist(countrylist,list_)\n",
        "    return list_\n",
        "\n",
        "  if indigenous_claim:\n",
        "    #print('In indigenous claim')\n",
        "    countrylist = extract_from_claim(indigenous_claim)\n",
        "    list_= checklist(countrylist,list_)\n",
        "    return list_\n",
        "\n",
        "  if country_of_origin_claims:\n",
        "    #print('In country of origin claim')\n",
        "    countrylist = extract_from_claim(country_of_origin_claims)\n",
        "    list_= checklist(countrylist,list_)\n",
        "    return list_\n",
        "\n",
        "  if country_claims:\n",
        "    #print('In country claim')\n",
        "    countrylist = extract_from_claim(country_claims)\n",
        "    list_= checklist(countrylist,list_)\n",
        "    return list_\n",
        "\n",
        "  else: #case in which we have no country of origin\n",
        "\n",
        "     desc = item.description['en'] #this is a string of the description,if we don't find the description from wikidata, we can access to the description given in the dataset\n",
        "     doc = nlp(desc)\n",
        "     gpe_list = extract_GPE(doc)\n",
        "     if len(gpe_list)>0:\n",
        "        #print('found gpe in the description!!!')\n",
        "        list_= checklist(gpe_list,list_)\n",
        "     else:\n",
        "      for ent in doc.ents:\n",
        "        if ent.label_=='PERSON':# Case is found a person or author\n",
        "          entid_aut = get_wikidata_id_from_name(ent.lemma_)\n",
        "          item_aut = client.get(entid_aut, load=True) #get the corresponding wikidata item\n",
        "          claims = item_aut.data.get(\"claims\", {}) #claims associatiated to item\n",
        "          origin_claim = claims.get(\"P27\",[])\n",
        "          countrylist = extract_from_claim(origin_claim)\n",
        "          #print('found gpe autor in description!!!')\n",
        "          list_= checklist(countrylist,list_)\n",
        "\n",
        "        elif (ent.label_ == 'NORP'): #Case in which is found citizenship and not country or language\n",
        "          entid_aut = get_wikidata_id_from_name(ent.lemma_)\n",
        "          item_aut = client.get(entid_aut, load=True) #get the corresponding wikidata item\n",
        "          claims = item_aut.data.get(\"claims\", {}) #claims associatiated to item\n",
        "          origin_claim = claims.get(\"P2341\",[])\n",
        "          countrylist = extract_from_claim(origin_claim)\n",
        "          #print('found gpe NORP in description!!!')\n",
        "          list_= checklist(countrylist,list_)\n",
        "          if len(countrylist)<1 and ent.text[0].isupper():\n",
        "            list_.append(ent.text)\n",
        "\n",
        "  return list_\n",
        "\n",
        "\n",
        "# determine if an item is popular given the id\n",
        "def popularity(item_id):\n",
        "  list_ = []\n",
        "  item = client.get(item_id, load=True) #item di wikidata\n",
        "  claims = item.data.get(\"claims\", {}) #claims associatiated to item\n",
        "  hashtag_claim = claims.get(\"P2572\", [])\n",
        "  unicode_character_claim = claims.get(\"P487\", [])\n",
        "  instagram_claim = claims.get(\"P2003\", [])\n",
        "  twitter_x_claim = claims.get(\"P2002\", [])\n",
        "  facebook_claim = claims.get(\"P2013\", [])\n",
        "  youtube_claim = claims.get(\"P2397\", [])\n",
        "  reddit_claim = claims.get(\"P11137\", [])\n",
        "\n",
        "  if(hashtag_claim or unicode_character_claim or instagram_claim or twitter_x_claim or facebook_claim or youtube_claim or reddit_claim):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "# Here we take the number of wikipedia pages available\n",
        "def get_wiki_languages(id_):\n",
        "  item = client.get(id_, load=True)\n",
        "  sitelinks = item.data.get(\"sitelinks\", {})\n",
        "  counter=0\n",
        "  for site_key, site_data in sitelinks.items():\n",
        "    if site_key.endswith(\"wiki\") and not site_key.startswith(\"commons\"):\n",
        "      lang = site_key.replace(\"wiki\", \"\")\n",
        "      title = site_data[\"title\"]\n",
        "      url = f\"https://{lang}.wikipedia.org/wiki/%7B{title.replace(' ', '')}\"\n",
        "      counter+=1\n",
        "  return counter\n",
        "\n",
        "\n",
        "# this function checks if an item is related to religion\n",
        "def religious(item_id):\n",
        "  list_ = []\n",
        "  item = client.get(item_id, load=True)\n",
        "  claims = item.data.get(\"claims\", {})\n",
        "  instance_claim = claims.get(\"P31\", [])\n",
        "  religion_claim = claims.get(\"P140\", [])\n",
        "  subclass_claim = claims.get(\"P279\", [])\n",
        "\n",
        " #here we indicated the instances and subclasses related to religion\n",
        "  list_instances = ['religious text','major religious groups','rite','Christianity' ]\n",
        "  list_subclasses = ['religious object','religious leader', ]\n",
        "\n",
        "  if instance_claim:\n",
        "    instance = extract_from_claim(instance_claim)\n",
        "    for elem in instance:\n",
        "      if elem.lower() in list_instances:\n",
        "        return 1\n",
        "\n",
        "  if religion_claim:\n",
        "     return 1\n",
        "\n",
        "  if subclass_claim:\n",
        "    try:\n",
        "      subclass = extract_from_claim(subclass_claim)\n",
        "      for elem in subclass:\n",
        "        if elem.lower() in list_subclasses:\n",
        "          return 1\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "  else:\n",
        "     desc = item.description['en']\n",
        "     doc = nlp(desc)\n",
        "     for elem in doc:\n",
        "       if elem.lemma_ == 'religious' or elem.lemma_ == 'religion':\n",
        "        return 1\n",
        "  return 0\n",
        "# this funciton check if an item, people and NORP entities contained in description are related to religion, using religious function\n",
        "def related_to_religion(item_id):\n",
        "  item = client.get(item_id, load=True)\n",
        "  r = religious(item_id)\n",
        "  if r == 1:\n",
        "    return 1\n",
        "  else:\n",
        "    desc = item.description['en']\n",
        "    doc = nlp(desc)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_=='PERSON':\n",
        "          entid = get_wikidata_id_from_name(ent.lemma_)\n",
        "          if entid is None:\n",
        "            pass\n",
        "          else:\n",
        "            r = religious(entid)\n",
        "            if r == 1:\n",
        "              return 1\n",
        "\n",
        "        elif (ent.label_ == 'NORP'):\n",
        "          entid = get_wikidata_id_from_name(ent.lemma_)\n",
        "          item = client.get(entid, load=True)\n",
        "          r = religious(entid)\n",
        "          if r == 1:\n",
        "            return 1\n",
        "  return 0\n",
        "\n"
      ],
      "metadata": {
        "id": "qGkUUB-3HRwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''this is the core function that classifies an entity id in:\n",
        "-cultural agnostic\n",
        "-cultural representative\n",
        "-cultural exclusive\n",
        "'''\n",
        "\n",
        "def classify_item(item_id):\n",
        "  time.sleep(5)#give time to wikidata API to refresh number of request otherwise crushes(highly suggested)\n",
        "  religious = related_to_religion(item_id)\n",
        "\n",
        "  if religious==1: #items/entities related to religion are considered cultural representative\n",
        "    return \"cultural representative\"\n",
        "\n",
        "  origin = where_is_it_from(item_id)\n",
        "  if origin==[]: #an item with no country of origin is agnostic\n",
        "    return \"cultural agnostic\"\n",
        "\n",
        "  elif len(origin)>1: #if an item has more countries of origin is considered representative\n",
        "    return \"cultural representative\"\n",
        "\n",
        "  else:\n",
        "    pop = popularity(item_id)\n",
        "    num_languages = get_wiki_languages(item_id)\n",
        "    history_pop = history_popularity(item_id)\n",
        "    adaptations = are_there_adaptations(item_id)\n",
        "\n",
        "    if pop==1: #if an item is known on the web\n",
        "      if num_languages>50 or history_pop==1 or adaptations:\n",
        "        '''if the item is either:\n",
        "         - translated in a lot of languages\n",
        "         - multiple geopolitical entities are mentioned in the history section\n",
        "         - is associated to media adaptation\n",
        "         then it is considered cultural representative. Exclusive otherwise. '''\n",
        "\n",
        "        return \"cultural representative\"\n",
        "      else:\n",
        "        return \"cultural exclusive\"\n",
        "\n",
        "    else: #item is not popular on social media\n",
        "      if num_languages>20 or history_pop==1:\n",
        "        ''' if the item is either:\n",
        "         - translated in a lot of languages (the threshold is lower in this case)\n",
        "         - multiple geopolitical entities are mentioned in the history section\n",
        "         then it is cultural representative. Exclusive otherwise\n",
        "        '''\n",
        "        return \"cultural representative\"\n",
        "      else:\n",
        "        return \"cultural exclusive\""
      ],
      "metadata": {
        "id": "0Rdf2EXcG-yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get predictions"
      ],
      "metadata": {
        "id": "B-6VPT9iQQE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate file .csv"
      ],
      "metadata": {
        "id": "Vjp8WMD54Zbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predition_from_dataset(dataset):\n",
        "  if response:\n",
        "    image = Image(data=response.content)\n",
        "    print('Waiting...')\n",
        "    display(image)\n",
        "  dataset = dataset.map(lambda example: {'label': classify_item(get_wikidata_id(example))})\n",
        "  clear_output()\n",
        "  return dataset\n",
        "\n",
        "eval_dataset_predictions = get_predition_from_dataset(dataset)"
      ],
      "metadata": {
        "id": "Oed9ljqiGJVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(eval_dataset_predictions)\n",
        "df.to_csv('Predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "Ul1qB1dISPwI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}